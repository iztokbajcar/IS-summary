\documentclass{article}

\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{hyperref}
\usepackage{multicol}

\author{ib8548}
\title{IS summary \\
    \large Summary of the Intelligent Systems course lectures at FRI}

\begin{document}
\maketitle \newpage
\tableofcontents \newpage

\section{Nature inspired computing}
    \subsection{Template of an evolutionary program}
    \begin{enumerate}
        \item Generate a population of agents (objects, data structures)
        \item Repeat:
            \begin{enumerate}[{2.1}]
                \item Compute \textbf{fitness} of the agents
                \item Select \textbf{candidates} for the reproduction (using fitness)
                \item Create new agents by \textbf{combining} the candidates
                \item \textbf{Replace} old agents with new ones
                \item Stop if satisfied
            \end{enumerate}
    \end{enumerate}

    \subsection{Key terms}
    \begin{itemize}
        \item \textbf{Individual} --- any possible solution
        \item \textbf{Population} --- a group of all individuals
        \item \textbf{Search space} --- all possible solutions to the problem
        \item \textbf{Chromosome} --- a blueprint for an individual
        \item \textbf{Trait} --- a possible aspect (features) of an individual
        \item \textbf{Allele} --- possible settings of a trait
        \item \textbf{Locus} --- the position of a gene on the Chromosome
        \item \textbf{Genome} --- a collection of all chromosomes for an individual
    \end{itemize}

    \subsection{Gene representation}
    \begin{itemize}
        \item Bit vectors
        \item Numeric vectors
        \item Strings
        \item Permutations
        \item Trees (representing functions, expressions, programs)
    \end{itemize}

    \subsection{Linear crossover}
    Let $\mathbf{x} = (x_1, \ldots, x_N)$ and $\mathbf{y} = (y_1, \ldots, y_N)$. \\
    Select $\alpha$ in $(0, 1)$. \\
    The result of the crossover is $\alpha \mathbf{x} + (1 - \alpha) \mathbf{y}$

        \subsubsection{Example}
        \begin{align*}
            \alpha &= 0.75 \\
            A &= (5, 1, 2, 10) \\
            B &= (2, 8, 4, 5) \\
        \end{align*}

        Crossover:

        \begin{align*}
            (\alpha * A_1 + (\alpha - 1) * B_1, \ldots, \alpha * A_4 + (\alpha - 1) * B_4) &= \\
            = (0.75 * 5 + 0.25 * 2, \ldots, 0.75 * 10 + 0.25 * 5) &= \\
            = (3.75 + 0.5, 0.75 + 2, 1.5 + 1, 7.5 + 1.25) &= \\
            = (4.25, 2.75, 2.5, 8.75)
        \end{align*}

        We can also choose a different $\alpha$ for each position. For example, using $\mathbf{\alpha} = (0.5, 0.25, 0.75, 0.5)$ would result in $(3.5, 6.25, 2.5, 7.5)$.

    \subsection{Gray coding of binary numbers}
    Each number differs from the previous by \textbf{one bit}. \\

    \begin{multicols*}{2}
        \begin{tabular}{|c|c|}  % chktex 44
            \hline  % chktex 44
            Binary & Gray \\
            \hline  % chktex 44
            0000 & \textbf{0000} \\
            0001 & \textbf{0001} \\
            0010 & \textbf{0011} \\
            0011 & \textbf{0010} \\
            0100 & \textbf{0110} \\
            0101 & \textbf{0101} \\
            0110 & \textbf{0100} \\
            0111 & \textbf{1100} \\
            1001 & \textbf{1101} \\
            1010 & \textbf{1111} \\
            1011 & \textbf{1110} \\
            1100 & \textbf{1010} \\
            1101 & \textbf{1011} \\
            1110 & \textbf{1011} \\
            1111 & \textbf{1000} \\
            \hline  % chktex 44
        \end{tabular} 

        \columnbreak 
        
        Constructing a n-bit Gray code recursively (example for $n = 2$):
        \begin{itemize}
            \item Create a (n-1) bit list: $\mathbf{0, 1}$
            \item Reflect: $\mathbf{1, 0}$
            \item Prefix old entries with $0$: $\mathbf{00, 01}$
            \item Prefix new entries with $1$: $\mathbf{11, 10}$
            \item Concatenate: $\mathbf{00, 01, 11, 10}$ 
        \end{itemize}    
    \end{multicols*}

    \subsection{Lamarckian mutation}
    \begin{itemize}
        \item Searches for the locally best mutation
    \end{itemize}

    \subsection{Gaussian mutation}
    \begin{itemize}
        \item Selects a position in the vector of floats and mutates it by adding a Gaussian error
    \end{itemize}

    \subsection{Selection}
    \begin{itemize}
        \item Proportional
        \item Rank proportional
        \item Tournament
        \item Single Tournament
        \item Stochastic universal sampling
    \end{itemize}

        \subsubsection{Proportional (roulette wheel) selection}
        Each individual gets a share on the ``wheel'' depending on its fitness. Fitter individuals get bigger shares. 

        \subsubsection{Tournament selection}
        \begin{enumerate}
            \item Set 
                $\mathbf{t} = \text{size of the tournament}$,
                $\mathbf{p} = \text{probability of a choice}$
            \item Randomly sample $t$ agents from the tournament population
            \item With probability $p$ select the best agent
            \item With probability $p(1 - p)$ select the second best agent
            \item With probability $p{(1-p)}^2$ select the third best agent
            \item \ldots
        \end{enumerate}

        \noindent The $\mathbf{n}$\textbf{-th} fittest agent is therefore selected with the probability of $\mathbf{p{(1-p)}^{(n-1)}}$ 

        \subsection{Stochastic universal sampling (SUS)}
        \begin{itemize}
            \item \textbf{Unbiased}
            \item Selecting $N$ agents with combined fitness of $F$
            \item Randomly chosen first position $r \in [0, \frac{F}{N}]$
            \item The positions $r + i * \frac{F}{N}; i \in 0, 1, \ldots, N-1$ determine the chosen agents
        \end{itemize}

        \subsection{Niche specialization}
        \begin{itemize}
            \item Punish too similar agents
        \end{itemize}

        \subsection{Stopping criteria}
        \begin{itemize}
            \item Number of generations
            \item Progress
            \item Availability of computational resources
            \item \ldots
        \end{itemize}

        \subsection{Parameters of genetic algorithms}
        \begin{itemize}
            \item Encoding
            \item Length of the Strings
            \item Size of the pupolation (from 20--50 to a few thousand)
            \item Selection method
            \item Probability of performing a crossover (usually high --- $\sim 0.9$)
            \item Probability of performing a mutation (usually low --- below $0.1$)
            \item Termination criteria (usually a number of generations or a target fitness)
        \end{itemize}

        \subsection{Pros and cons of GA}
        Pros:
        \begin{itemize}
            \item \textbf{Low} time and memory \textbf{requirements} compared to searching a very large feature space
            \item A solution can be found without any explicit analytical work
        \end{itemize}
        Cons:
        \begin{itemize}
            \item Randomized and therefore not optimal
            \item Can get stuck on local maxima
            \item We have to figure out how to represent candidates with the available methods (e.g.\ as a bit string)
        \end{itemize}
    \newpage

\section{Predictive modelling}
    \subsection{Learning}
    Acquiring new, modifiying and/or reinforcing existing:
    \begin{itemize}
        \item Knowledge
        \item Behaviors
        \item Skills
        \item Values
        \item Preferences
    \end{itemize}
    \textbf{Statistical learning} deals with finding a predictive function based on the data.
    
    \subsection{Notation}
    \begin{align*}
        X &= \begin{pmatrix}
            X_1 \\
            X_2 \\
            X_3
        \end{pmatrix} \\
        Y &= f(X) + \varepsilon \\
        Y_i &= f(X_i) + \varepsilon
    \end{align*}

    $\varepsilon$ --- measurement errors, its mean is 0 and it is independent from $X$.

    \subsection{Goals of learning}
    \begin{itemize}
        \item Prediction
        \item Inference
    \end{itemize}

    \subsection{Prediction}
    \begin{itemize}
        \item If we have a good estimate for $f$, we can make accurate predictions for $Y$ based on a new value of $X$.
    \end{itemize}

    \subsection{Inference}
    \begin{itemize}
        \item Finding out the relationship between $Y$ and $X$.
        \item Sometimes more important than prediction (for example in medicine)
    \end{itemize}

    \subsection{Parametric methods}
    \begin{itemize}
        \item We estimate $f$ by estimating the set of parameters
    \end{itemize}

    \begin{enumerate}
        \item Come up with a model based on assumptions about the form of $f$, e.g.\ a linear model: \\
        \[
            f(\mathbf{X_i} = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \cdots + \beta_p X_{ip})
        \]
        \begin{itemize}
            \item More complicated and flexible models for $f$ are often more realistic
        \end{itemize}
        \item Use the training data to fit the model (estimate $f$) --- e.g.\ the least squares method in the case of linear models
    \end{enumerate}

    \subsection{Non-parametric methods}
    \begin{itemize}
        \item They do not make assumptions about the funct.\ form of $f$. They fit a wider range of possible shapes, but require a large number of observations compared to parametric methods.
    \end{itemize}

    \subsection{Types of learning}
    \begin{itemize}
        \item Supervised
        \item Unsupervised
        \item Semi-Supervised
        \item Self-Supervised
        \item Weakly-supervised
    \end{itemize}

        \subsubsection{Supervised learning}
        Both the predictors ($X_i$) and the response ($Y_i$) are given.

        \subsubsection{Unsupervised learning}
        We don't know $Y$ --- we are looking for similarities between attributes ($X$).

        \subsubsection{Semi-supervised learning}
        Only a small sample of labelled instances is observed, but a large set of instances is unlabelled.
        \begin{itemize}
            \item A supervised model is used to label unlabelled instances
            \item The most reliable predictions are then added to the training set for the next iteration of supervised learning
        \end{itemize}

        \subsubsection{Self-supervised learning}
        \begin{itemize}
            \item A mixture of supervised and unsupervised learning
            \item Learns from unlabelled data
            \item The labels are obtained \textbf{from related properties of the data}
        \end{itemize}

        Examples: 
        \begin{itemize}
            \item in NLP: predicting hidden words in a sentence based on the remaining words
            \item in video processing: predicting past or future frames from the observed ones
        \end{itemize}

        \subsubsection{Weakly-supervised data}
        Using imprecise or noisy sources to supervise labelling large amounts of training data, then performing supervised learning. \\
        Example: using a smart electricity meter to estimate household occupancy
    
    \subsection{Criteria of success for machine learning}
    How to select the best model? Most popular criterions:
    \begin{itemize}
        \item For \textbf{regression}: mean squared error (MSE)
        \[
            MSE = \frac{1}{n} \sum_{i=1}^{n} {(y_i - f'(x_i))}^2
        \]
        \item For \textbf{classification}: classification accuracy (CA)
        \[
            CA = \frac{1}{n} \sum_{i=1}^{n} I(y_i = y'_i)
        \]
    \end{itemize}

    \subsection{No free lunch theorem}
    If no information about $f(X)$ is provided:
    \begin{itemize}
        \item No classifier is better than some other in the general case
        \item No classifier is better than random in the general case
    \end{itemize}

    \newpage

\section{Bias, variance and predictive models}
    \subsection{Bias}
    \begin{itemize}
        \item It is the error that is introduced by modelling a (usually very complicated) real life problem by a much simpler problem.
        \item Example: linear regression assumes a linear relationship between $Y$ and $X$; in reality that is unlikely to happen.
        \item Generally, the \textbf{more flexible/complex} a method is, the \textbf{less} bias it will have.
    \end{itemize}

    \subsection{Variance}
    \begin{itemize}
        \item Refers to how much our estimate for $f$ would change if we had a different training data set
        \item Generally, the \textbf{more flexible} a method is, the \textbf{more} variance it has.
    \end{itemize}

    \subsection{Bayes error rate}
    \begin{itemize}
        \item Refers to the lowest possible error rate that could be achieved (if we knew the true probability distribution)
        \item On test data, no classifier can get lower error rates than this
        \item It can't be calculated exactly in real-life problems
    \end{itemize}

    \subsection{Bayes optimal classifier}
    \begin{itemize}
        \item For a new $x_0$, returns the maximally probable prediction value \\ $P(Y=y \vert X=x_0)$
        \item In classification: $argmax_j P(Y=y_j \vert X=x_0)$
    \end{itemize}

    \subsection{k-Nearest Neighbors (KNN)}
    \begin{itemize}
        \item For any given $X$ we find the closest $k$ neighbors in the training data and examine their corresponding $Y$
        \item The \textbf{smaller} $k$, the \textbf{more flexible} the method will be
    \end{itemize}

    \subsection{k-NN classifier}
        \subsubsection{For classification}
        \begin{itemize}
            \item $P(Y = j \vert X = x_0) = \frac{1}{K} \sum_{i \in \mathcal{N}_0} I(y_i = j)$
        \end{itemize}

        \subsubsection{For regression}
        \begin{itemize}
            \item $f(x) = \frac{1}{k} \sum_{x_i \in N_i} y_i$
        \end{itemize}

    \subsection{Types of bias}
    \begin{itemize}
        \item Reporting bias
        \item Automation bias
        \item Selection bias
        \item Group attribution bias
        \item Implicit bias
    \end{itemize}

        \subsubsection{Reporting bias}
        \begin{itemize}
            \item When the frequency of events does not accurately reflect their real-world frequency
            \item Because people tend to focus on documenting unusual or especially memorable circumstances
        \end{itemize}
        \textit{Example:} The majority of book reviews are extreme opinions (very positive or very negative), because people were more inclined to giving a review if they had a strong opinion of the book

        \subsubsection{Automation bias}
        \begin{itemize}
            \item The tendency to prefer automated systems over non-automated ones
        \end{itemize}
        \textit{Example:} Engineers develop a new model for identifying product defects to relieve the burden off inspectors, but its results are found out to be worse than those of human inspectors

        \subsubsection{Selection bias}
        \begin{itemize}
            \item Occurs if the dataset's examples are chosen irrepresentatively
        \end{itemize}

        \subsubsection{Group attribution bias}
        \begin{itemize}
            \item Generalizing properties of individuals to the entire group they belong to
            \begin{itemize}
                \item In-group bias: you \textbf{also belong} to the group \\
                \textit{Example:} Presuming that people who study at FRI are better at computer science than students of other computer science faculties 
                \item Out-group bias: you \textbf{don't belong} to the group \\
                \textit{Example:} Making the presumption that people who did not attend a computer science academy do not have sufficient expertise in the field
            \end{itemize} 
        \end{itemize}

        \subsubsection{Implicit bias}
        \begin{itemize}
            \item Occurs when assumptions are made based on an individual's mental models and personal experiences that do not apply more generally
        \end{itemize}

    \newpage

\section{Feature selection}
    \subsection{Types of feature selection methods}
    \begin{itemize}
        \item \textbf{Filter methods:} independent of the learning algorithm, selecting the most discriminative features based on the character of data (information gain, ReliefF)
        \item \textbf{Wrapper methods:} using the intended learning algorithm to evaluate the features (e.g.\ progressively adding features while performance increases)
        \item \textbf{Embedded methods:} selecting features during the process of learning
    \end{itemize}

    \subsection{Heuristic measures for attribute evaluation}
    \begin{itemize}
        \item Impurity based --- assuming conditional independence between the attributes
        \begin{itemize}
            \item Information theory based (information gain, gain ratio, distance measure, J-measure)
            \item Probability based (Gini index, DKM, classification error)
            \item MDL
            \item G, $\aleph^2$
            \item Mean squared error (MSE), mean absolute error (MAE)
        \end{itemize}
        \item Context-sensitive measures --- affinity graph based
        \begin{itemize}
            \item Relief, Contextual Merit
            \item Random forests or boosting based evaluation
        \end{itemize}
    \end{itemize}

        \subsubsection{Information gain}
        \[ I(\tau) = - \sum_{i=1}^{c} p(\tau_i) \log_2 p(\tau_i) \]
        \[ I(\tau \vert A) = - \sum_{j=1}^{v_A} p(v_j) \sum_{i=1}^{c} p(\tau_i \vert v_j) \log_2 p(\tau_i \vert v_j) \]
        \[ IG(A) = I(\tau) - I(\tau \vert A) \]
        Measures the purity of labels before and after the split, with each attribute evaluated independently from the others.

        \subsubsection{Relief algorithms}
        \begin{itemize}
            \item Criterion: evaluate attributes according to their power of separation between near instances
            \item Values of a good atribute should:
            \begin{itemize}
                \item \textbf{Distinguish} between near instances from \textbf{different} class
                \item Have \textbf{similar} values to near instances from the \textbf{same} class
            \end{itemize}
            \item These algorithms take no assumptions about conditional independence, but are reliable also in problems with strong conditional dependencies
            \item Extension: \textbf{ReliefF}
            \begin{itemize}
                \item Multi-class problems
                \item Incomplete and noisy data
                \item Robust
            \end{itemize}
        \end{itemize}

    \subsection{Ridge regression}
    \begin{itemize}
        \item Ordinary Least Squares (OLS) minimizes:
        \[ RSS = \sum_{i=1}^{n} \left(y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{ij} \right) \]
        \item Ridge regression uses a slightly different equation --- it minimizes the following expression:
        \[ RSS + \lambda \sum_{j=1}^{p} \beta_j^2 \]
        \item This has the effect of ``shrinking'' large values of $\beta$s towards zero (we added a penalty term to the OLS equation)
        \item The penalty term \textbf{introduces bias} but can substantially \textbf{reduce variance} --- there is a bias/variance tradeoff
    \end{itemize}

    \subsection{The LASSO method}
    \begin{itemize}
        \item Because the penalty term in ridge regression will never force any of the coefficients to be zero, using ridge regression wil cause the final model to include all variables, which makes it hard to interpret
        \item LASSO fixes this problem by using a different penalty term:
        \[ RSS + \lambda \sum_{j=1}^{p} |\beta_j| \]
        \item Using LASSO therefore enables us to produce a model that has high predictive power, but is simple to interpret
    \end{itemize}

    \subsection{Wrapper approach}
    \begin{enumerate}
        \item Start with an empty set of features $S= \emptyset$
        \item Repeat until all features are added to $S$:
        \begin{enumerate}[{2.1}]
            \item Add all unused features one by one to $S$
            \item Train a prediction model with each set $S$
            \item Evaluate each prediction model
            \item Keep the best added feature in $S$
        \end{enumerate}
        \item Return the best set of features encountered
    \end{enumerate}

    \subsection{Confusion matrix}
    \begin{tabular}{|c|c c|c|}  % chktex 44
        \hline  % chktex 44
        A $\backslash$ P & $C$ & $\neg{C}$ & \\
        \hline   % chktex 44
        $C$ & \textbf{TP} & \textbf{FN} & \textbf{P} \\
        $\neg{C}$ & \textbf{FP} & \textbf{TN} & \textbf{N} \\
        \hline  % chktex 44
        & \textbf{P'} & \textbf{N'} & All \\
        \hline  % chktex 44
    \end{tabular}

    \subsection{Model evaluation metrics}
    \begin{itemize}
        \item Regression: MSE, MAE
        \item Classification: accuracy, sensitivity, specificity, AUC, precision, recall
    \end{itemize}

        \subsubsection{Classification accuracy}
        \[ \text{Accuracy} = \frac{\text{TP} + \text{TN}}{\text{All}} \]
        The ratio of test set tuples that are correctly classified.

        \subsubsection{Sensitivity and specificity}
        \textbf{Sensitivity:} True Positive recognition rate
        \[ \text{Sensitivity} = \frac{\text{TP}}{\text{P}} \]
        \textbf{Specificity:} True Negative recognition rate
        \[ \text{Sensitivity} = \frac{\text{TN}}{\text{N}} \]

        \subsubsection{Precision and recall}
        \textbf{Precision:} \textit{exactness} --- what percentage of positively classified tuples are actually positive
        \[ \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}} \]
        \textbf{Recall:} \textit{completeness} --- what percentage of positive tuples the classifier labelled as positive
        \[ \text{Recall} = \frac{\text {TP}}{\text{TP} + \text{FN}} \]
        There is an inverse relationship between precision and recall, the perfect score is 1.0

        \subsubsection{F-measures}
        \textbf{F-measure ($F_1$ or F-score):} a harmonic mean of precision and recall
        \[ F = \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} \]
        $\mathbf{F_\beta}\textbf{:}$ a weighted measure of precision and recall (assigns $\beta$ times as much weight to recall as to precision)
        \[ F_\beta = \frac{(1 + \beta^2) \times \text{Precision} \times \text{Recall}}{\beta^2 \times \text{Precision} + \text{Recall}} \]

        \subsubsection{ROC curve}
        \begin{itemize}
            \item Shows both TP rate and FP rate simultaneously
            \item To summarize overall performance, we also use \textit{area under the ROC curve} (AOC)
            \item The larger the AUC, the better the classifier
        \end{itemize}

    \subsection{Unsupervised feature selection}
    \begin{itemize}
        \item Criterion: preserve similarity between instances
        \item SPEC:\@ spectral feature selection
    \end{itemize}

    \newpage

\section{Ensemble methods}
    \begin{itemize}
        \item Learn a large number of basic (simple) classifiers
        \item Merge their predictions
    \end{itemize}

    The most successful methods:
    \begin{itemize}
        \item Bagging
        \item Boosting
        \item Random forest
    \end{itemize}

        \subsection{Bagging}
        \begin{itemize}
            \item \textbf{B}ootstrap \textbf{agg}regat\textbf{ing}
            \item Solves the problem of decision trees suffering from high variance
            \item Based on:
            \begin{itemize}
                \item Averaging --- reduces variance
                \item Bootstrapping --- plenty of training datasets
            \end{itemize}
            \item Procedure:
            \begin{enumerate}
                \item Generate $B$ different bootstrapped training datasets
                \item Train the statistical learning method on each of the $B$ training \\ datasets and obtain the prediction
            \end{enumerate}
        \end{itemize}

            \subsubsection{Bootstrapping}
            \begin{itemize}
                \item Resampling of the observed dataset (each dataset is of equal size to the observed dataset)
                \item Draw instances from a dataset \textbf{with replacement} (tuples may repeat)
            \end{itemize}

            \subsubsection{Bagging for regression trees}
            \begin{enumerate}
                \item Construct $B$ regression trees using $B$ bootstrapped training datasets
                \item Average the resulting predictions
            \end{enumerate}
            Averaging these trees reduces variance, thus we end up lowering both variance and bias

            \subsubsection{Bagging for classification trees}
            \begin{enumerate}
                \item Construct $B$ decision trees using $B$ bootstrapped training datasets
                \item For prediction, there are two approaches:
                \begin{itemize}
                    \item \textbf{Majority vote:} record the class that each bootstrapped data set predicts and provide an overall prediction to the most commonly occurring one
                    \item Average the probabilities and then predict to the class with the highest priority
                \end{itemize}
            \end{enumerate}

        \subsection{Random forests}
        \begin{itemize}
            \item A very efficient statistical learning method
            \item Builds on the idea of bagging, but provides an improvement by \\ de-correlating the trees
        \end{itemize}

        \begin{enumerate}
            \item Build a number of decision trees on bootstrapped training sample
            \item When building these trees, each time a split in a tree is considered, a random sample of $m$ predictors is chosen as split candidates from the full set of $p$ predictors
        \end{enumerate}

        Properties:
        \begin{itemize}
            \item Low classification/regression error
            \item No overfitting
            \item Robust
            \item Relatively fast
            \item Instances that are not selected with bootstrap replication are used for the evaluation of the tree (\textbf{OOB --- Out-Of-Bag evaluation})
        \end{itemize}

        \subsection{Out-of-bag evaluation}
        \begin{itemize}
            \item On average, $\sim 37\%$ of the learning set is not used to train each of the basic classifiers
        \end{itemize}

        \subsection{Boosting}
        \begin{itemize}
            \item Grows the tree sequentially: each added tree uses information about errors of previous trees
        \end{itemize}

        \subsection{Weighting of the trees}
        \begin{itemize}
            \item Not all trees are equally important
            \item Weight the trees according to the data
            \item Assume linear combination of base coefficients
        \end{itemize}

        \subsection{MARS --- Multivariate Adaptive Regression Splines}
        \begin{itemize}
            \item Generalization of stepwise linear regression
            \item Modification of trees to improve regression performance
            \item Able to capture additive structures
            \item Not tree-based
            \item Set $C$ represents a candidate set of linear splines with ``knees'' at each data point $X_i$
            \item Models are built with elements from $C$ or their products
            \[ C = {\{{(X_j - t)}_+, {(t - X_j)}_+ \}}_{t \ni} x_{1j}, x_{2j}, \ldots, x_{Nj}, j = 1, 2, \ldots, p \]
            \item Model form:
            \[ f(X) = \beta_0 + \sum_{m=1}^{M} \beta_m h_m(X) \]
        \end{itemize}

        Procedure:
        \begin{enumerate}
            \item Given a choice for the $h_m$, the coefficients $\beta$ are chosen by the standard linear regression
            \item Start with $h_0(X) = 1$; all functions in $C$ are candidate functions
            \item At each stage, consider as a new basis function pair all products of a function $h_m$ in the model set $M$, with one of the reflected pairs in $C$
            \[ \beta_{M+1} h_l(X) * {(X_j - t)}_+ + \beta_{M+2} h_l(X) * {(t - X_j)}_+, h_l \in M \]
            \item We add to the model terms of the form: 
            \[ h_m(X) * {(t - X_j)}_+ \qquad h_m(X) * {(X_j - t)}_+ \]
        \end{enumerate}

    \newpage

\section{Kernel methods}
    \subsection{Support vector machines}
    \begin{itemize}
        \item Idea: finding a hyperplane that gives the biggest separation between the classes
        \item In practice, it is usually not possible to find a hyperplane that perfectly separates two classes
        \item We try to find the best separation
    \end{itemize}

        \subsubsection{Non-linear support vector classifier}
        \[ Y_i = \beta_0 + \beta1 b_1 (X_i) + \beta_2 b_2 (X_i) + \cdots + \beta_p b_p (X_i) + \varepsilon_i \]

        \subsubsection{Example}
        \begin{itemize}
            \item Suppose we use polynomials as bases
            \[ X_1,\; X_2,\; X_1^2,\; X_2^2,\; X_1 X_2,\; X_1^3,\; X_1 X_2^2,\; \ldots \]
            \item We go from $p$-dimensional space to $M > p$-dimensional space and fit the SVM classifier in the enlarged space
            \item For the bases $(X_1,\; X_2,\; X_1^2,\; X_2^2,\; X_1 X_2)$, this gives a non-linear classifier in the original space
            \[ \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1 X_2 = 0 \]
        \end{itemize}

        \subsubsection{Common kernel functions}
        Instead of choosing the basis, we use a kernel function.
        Common kernel functions include:
        \begin{itemize}
            \item Linear
            \item Polynomials
            \item Radial basis
            \item Sigmoid
        \end{itemize}

        \subsubsection{SVM for more than one class}
        \begin{itemize}
            \item OVA --- One versus All
            \begin{itemize}
                \item Fit $K$ different 2-class SVM classifiers, each class versus the rest
                \item Classify new $x$ to the class for which the value of the kernel function is the largest.
            \end{itemize}

            \item OVO --- One versus One
            \begin{itemize}
                \item Fit all $\binom{K}{2}$ pairwise classifiers
                \item Classify new $x$ ti the class that wins the most pairwise competitions
            \end{itemize}

            \item If $K$ is not too large, use OVO
        \end{itemize}

    \newpage

\section{Neural networks}
    \subsection{Neuron}
    \begin{itemize}
        \item Computational units
        \item Passing messages (information) in the network
        \item Typically organized into layers
    \end{itemize}

    \subsection{Activation functions}
    \begin{itemize}
        \item Step function: $f(x) = \begin{cases}
            1, & x > 0 \\
            0, & x \leq 0
        \end{cases}$
        \item Sigmoid (logistic) function: $f(x) = \frac{1}{1 + e^{-x}}$
        \item ReLU (rectified linear unit): $f(x) = \max(0, x)$
        \item Softplus / approximation of ReLU with \\ continuous derivation: $f(x) = \ln(1 + e^x)$
        \item ELU (Exponential Linear Unit): $ELU(x) = \begin{cases}
            c * (e^x - 1) & \text{for } x < 0 \\
            x & \text{for } x \geq 0
        \end{cases}$
    \end{itemize}

    \subsection{Multi-layeres NNs}
    \begin{enumerate}
        \item The \textbf{inputs} to the network correspond to the attributes measured for each training tuple
        \item Inputs are fed simultaneously into the units making up the \textbf{input layer}
        \item They are then \textbf{weighted} and fed simultaneously to a \textbf{hidden layer}
        \item The number of hidden layers is arbitrary; if more than 1 hidden layer is used, the network is called a \textbf{deep neural network}
        \item The weighted outputs of the last hidden layer are inputs to units making up the \textbf{output layer},  which emits the network's prediction
        \item The network is \textbf{feed-forward}: none of the weights cycle back to an input unit or to an output unit of a previous layer
        \item If we have backwards connections, the network is called a \textbf{recurrent neural network}
        \item From a stytistical point of view, networks perform \textbf{non-linear regression}: given enough hidden units and enough training samples, they can closely approximate any function
    \end{enumerate}

    \subsection{Backpropagation learning algorithm for NN}
    \begin{itemize}
        \item Backpropagation: a neural network learning algorithm
        \item Started by psychologists and neurobiologists to develop and test computational analogues of neurons
        \item During the learning phase, the network learns by adjusting the weights so as to be able to predict the correct class label of the input tuples
        \item Also referred to as \textbf{connectionist learning} due to the connections between units
    \end{itemize}
    
    \subsection{Softmax}
    \begin{itemize}
        \item Normalizes the output scores to be a probability distribution (values between 0 and 1, the sum is 1)
        \[ y_i = \frac{e^{z_i}}{\sum_{j \in \text{group}} e^{z_j}} \]
        \[ \frac{\partial y_i}{\partial z_i} = y_i (1 - y_i) \]
    \end{itemize}

    \subsection{Criterion function}
    \begin{itemize}
        \item Together with softmax we frequently use cross entropy as the cost function $C$
        \[ C = - \sum_j t_j \log y_j \]
        \[ \frac{\partial C}{\partial z_i} = \sum_j \frac{\partial C}{\partial y_j} \frac{\partial y_j}{\partial z_i} = y_i - t_i \]
    \end{itemize}

    \subsection{Backpropagation}
    \begin{itemize}
        \item Iteratively process a set of training tuples and compare the network's prediction with the actual known target value
        \item For each training tuple, the weights are modified to \textbf{minimize the mean squared error} between the network's prediction and the actual target value
        \item Modifications are made in the ``\textbf{backwards}'' direction: from the output layer, through each hidden layer down to the first hidden layer (hence ``backpropagation'')
        \item Steps:
        \begin{enumerate}
            \item Initialize weights to small random numbers, associated with biases
            \item Propagate the inputs forward (by applying activation function)
            \item Backpropagate the error (by updating weights and biases)
            \item Terminating condition: when the error is very small, etc.
        \end{enumerate}
    \end{itemize}

    \subsection{Error backpropagation}
    \begin{itemize}
        \item Performing gradient descent on the whole network
        \item Training will procees from the last layer to the first
        \item introduce variables over the neura network
        \[ \vec{\theta} = \{w_{ij}, w_{jk}, w_{kl}\} \]
        \begin{itemize}
            \item Distinguish the input and output of each node
        \end{itemize}
    \end{itemize}

    \subsection{Autoencoders}
    \begin{itemize}
        \item Designed to reproduce their input, especially for images
        \item The key point is to reproduce the input from a learned encoding
        \item The loss function is the reproduction error
    \end{itemize}

        \subsubsection{Structure}
        \begin{itemize}
            \item \textbf{Encoder:} compresses input into a latent-space of usually smaller dimension. $h = f(x)$
            \item \textbf{Decoder:} reconstructs input from the latent space. $r = g(f(x))$ with $r$ as close to $x$ as possible
        \end{itemize}

        \subsubsection{Denoising autoencoders}
        \begin{itemize}
            \item Basic autoencoders train to minimize the loss between $x$ and the reconstruction, $g(f(x))$
            \item Denoising autoencoders train to minimize the loss between $x$ and $g(f(x+w))$, where $w$ is random noise
            \item Same possible architectures, different training data
        \end{itemize}

        \subsubsection{Properties of autoencoders}
        \begin{itemize}
            \item \textbf{Data-specific:} autoencoders are only able to compress data similar to what they have been trained on
            \item \textbf{Lossy:} the decompressed outputs will be degraded compared to the original inputs
            \item \textbf{Learned automatically from examples:} It is easy to train specialized instances of the algorithm that will perform well on a specific type of input
        \end{itemize}

        \subsection{Generative Adversarial Networks (GANs)}
        \begin{itemize}
            \item Learn a generative model
            \item Are trained in an adversarial setting
            \item Use deep neural networks
        \end{itemize}

        \subsection{Adversarial training}
        \begin{itemize}
            \item \textbf{Generator:} generates fake samples and tries to fool the discriminator
            \item \textbf{Discriminator:} tries to distinguish between real and fake samples
            \item We train them against each other
            \item By repeating the training process, the generator and discriminator improve
        \end{itemize}

    \newpage

\section{Inference and explanation of prediction models}
    \subsection{Visualization}
    \begin{itemize}
        \item Helps us understand the data we are working with
        \item Due to limitations of human visual perception, we often see what we want to see, therefore it is useful to use visualizations which expose ``the unknown''
    \end{itemize}

    \subsection{Explanation of predictions}
    \begin{itemize}
        \item A number of successful prediction algorithms exist (SVM, boosting, random forests, neural networks), but they are a \textbf{black box} to the user
        \item There are many fields where users are concerned about the transparency of the models (medicine, law, consultancy, \ldots)
    \end{itemize}

    \subsection{Domain level explanation}
    \begin{itemize}
        \item Trying to explain the ``true causes and effects''
        \item Usually unreachable
        \item Some aspects are covered with attribute evaluation, detection of redundancies, \ldots
        \item Targeted indirectly through models
    \end{itemize}

    \subsection{Model-based explanations}
    \begin{itemize}
        \item They make the prediction process of a particular problem transparent
        \item The explanation correctness is independent of the correctness of the prediction
        \item Better models enable (in principle) better explanations at the domain level
        \item We are mostly interested only in the explanation at the model level
    \end{itemize}

    \subsection{Types of explanation techniques}
    \begin{itemize}
        \item Model-specific
        \begin{itemize}
            \item Especially used for deep neural networks
        \end{itemize}
        \item Model-agnostic
        \begin{itemize}
            \item Can be used for any predictor
            \item Based on perturbation of the inputs
        \end{itemize}
    \end{itemize}

    \subsection{Perturbation-based explanations}
    \begin{itemize}
        \item Importance of a feature or a group of features in a specific mode can be estimated by simulating lack of knowledge about the values of the feature(s)  % chktex 36
    \end{itemize}

    \subsection{Instance-level explanations}
    \begin{itemize}
        \item They explain prediction for each instance separately
        \begin{itemize}
            \item This is what \textbf{practitioners} applying the models are interested in
        \end{itemize}
        \item Model-based
    \end{itemize}

    \subsection{Model-level explanations}
    \begin{itemize}
        \item They show the overall picture of a problem the model conveys
        \begin{itemize}
            \item This is what \textbf{knowledge extractors} are interested in
        \end{itemize}
        \item Model-based
    \end{itemize}

    \subsection{The EXPLAIN method}
    \begin{itemize}
        \item ``Hide'' one attribute at a time
        \item Estimate the contribution of the attribute:
        \[ p(y_k \vert x) - p_{S \backslash \{i\}}  (y_k \vert x) \]
        \item Assume an instance $(\mathbf{x}, y)$, where components of $\mathbf{x}$ are values of attribute $A_i$
        \item For a new instance $\mathbf{x}$, we want to know what role each attribute's value plays in the prediction model $f$
        \item For that purpose,
        \begin{itemize}
            \item We compute $f(\mathbf{x} \backslash A_i)$, the model's prediction for $\mathbf{x}$ without the knowledge of the event $A_i = a_k$ (marginal prediction)
            \item We compare $f(\mathbf{x})$ and $f(\mathbf{x} \backslash A_i)$ to assess the importance of $A_i = a_k$
            \item The larger the difference, the more important the role of $A_i = a_k$ in the model
        \end{itemize}
        \item $f(\mathbf{x})$ and $f(\mathbf{x} \backslash A_i)$ are the source of explanations
    \end{itemize}

    \subsection{Evaluation of prediction differences}
    \begin{enumerate}
        \item Difference of probabilities
        \[ \text{probDiff}_i(y \vert \mathbf{x}) = p(y \vert \mathbf{x}) - p(y \vert \mathbf{x} \backslash A_i) \]
        \item Information gain
        \[ \text{infGain}_i(y \vert \mathbf{x}) = \log_2 p(y \vert \mathbf{x}) - \log_2 p (y \vert \mathbf{x} \backslash A_i)\]
        \item Weight of evidence (also log odds ratio)
        \[ odds(z) = \frac{p(z)}{1 - p(z)} \]
        \[ WE_i(y \vert \mathbf{x}) = \log_2 odds(y \vert \mathbf{x}) - \log_2 odds(y \vert \mathbf{x} \backslash A_i) \] 
    \end{enumerate}

        \subsubsection{Implementation}
        \begin{itemize}
            \item $p(y \vert \mathbf{x})$: classify $\mathbf{x}$ with the model
            \item $p(y \vert \mathbf{x} \backslash A_i)$: simulate lack of knowledge of $A_i$ in the model
            \begin{itemize}
                \item Replace with special NA value: good for some, but mostly bad
                \item Average prediction across perturbations of $A_i$
                \[ p(y \vert \mathbf{x} \backslash A_i) = \sum_a p(A_i = a_s) p(y \vert \mathbf{x} \leftarrow A_i = a_s) \]
                \item Use discretization for numeric attributes
                \item Use Laplace correction for probability estimation
            \end{itemize}
        \end{itemize}

    \subsection{The IME method}
    \begin{itemize}
        \item Interactions-based Method for Explanation
        \item ``Hide' any subset of attributes at a time ($2^a$ subsets!)
        \item The \textbf{source of explanations} is the difference in prediction using a subset of features $Q$ and an empty set of features $\emptyset$
        \[ \Delta Q = h(x_Q) - h(x_\emptyset) \]
        \item The feature gets some credit for standalone contributions and for contributions in interactions
    \end{itemize}

    \subsection{The LIME explanation method}
    \begin{itemize}
        \item Optimize a trade-off between local fidelity of explanation and its interpretability
        \item LIME samples around the explanation instance $x$ to draw samples $z$ weighted by the distance $\pi(x, z)$
        \item Samples $z$ are used to train an interpretable model $g$ (linear model)
        \item Faster than IME
    \end{itemize}

    \subsection{The SHAP method}
    \begin{itemize}
        \item SHapley Additive exPlanation
        \item Unification of several explanation methods, including IME and LIME
        \item Faster than IME, but still uses a linear model with all its strengths and weaknesses
    \end{itemize}

    \newpage

\section{Natural language processing}
    \subsection{Uses}
    \begin{itemize}
        \item Speech recognition and synthesis
        \item Automatic reply Engineers
        \item Machine translation
        \item Text summarization
        \item Question answering
        \item Language generations
        \item Interface to databases
        \item Intelligent search and information extraction
        \item Sentiment detection
        \item Semantic analysis
        \item Named entity recognition and linking
        \item Categorization, classification of documents, messages, etc.
        \item Many tools and language resources
        \item Prevalence of deep neural network approaches
        \item Cross-lingual approaches
    \end{itemize}

    \subsection{Linguistic analysis}
    \begin{itemize}
        \item \textbf{Prosody} --- the patterns of stress and intonation in a language
        \item \textbf{Phonology} --- systems of sounds and relationships among the speech souds that constitute the fundamental components of a language
        \item \textbf{Morphology} --- the admissin+ble arrangements of sounds in words; how to form words, prefixes and suffixes\ldots
        \item \textbf{Syntax} --- the arrangement of words and phrases to reate well-formed sentences in a language
        \item \textbf{Semantics} --- the meaning of a word, phrase, sentence or text
        \item \textbf{Pragmatics} --- language in use and the contexts in which it is used, taking turns in conversation, text organization, presupposition and implicature
        \item Knowing the world: knowledge of the physical word, humans, society, intentions in communications\ldots
    \end{itemize}

    \subsection{The classic approach}
    \begin{enumerate}
        \setcounter{enumi}{-1}
        \item Text preprocessing
        \item Syntactic analysis
        \item Semantic interpretation
        \item Use of world knowledge
    \end{enumerate}

    \subsection{Lemmatization and stemming}
    \begin{itemize}
        \item \textbf{Lemmatization:} the process of grouping together the different forms of a word into a single item
        \item The stemmer operates on a single word without knowledge of its context
        \item Lemmatization difficulty is language dependent
        \item Usage of rules and dictionaries
    \end{itemize}

    \subsection{Part of speech (POS) tagging}
    \begin{itemize}
        \item Assigning the correct part of speech (noun, verb, \ldots) to words
        \item Helps recognize phrases and names
        \item Uses rues and machine learning models
    \end{itemize}

    \subsection{Named entity recognition (NER)}
    \begin{itemize}
        \item Recognizing named entities, such as names of organizations (e.g.\ NATO), people (e.g.\ Jens Stoltenberg), countries \ldots
        \item Named entity linking (NEL) links same words with different meaning (e.g.\ jaguar, Paris) to an unique identifier (wikification)
    \end{itemize}

    \subsection{Syntax analysis}
    \begin{itemize}
        \item \textbf{The 1.\ phase of text understanding}
        \item POS tagging
        \item Determining the words' roles in the sentence (subject, object, predicate)
        \item The result is mostly presented in a form of a parse tree
        \item Syntax, morphology and some semantics are needed
    \end{itemize}

    \subsection{n-gram tagging}
    \begin{itemize}
        \item Contect of $n-1$ preceding words
        \item Corpus-based learning
        \item Markov models, HMM, learning with EM
        \[ t_i = \arg\max_j P(t^{(j)} \vert t_{i-1}) * P(w_i \vert t^{(j)}) \]
    \end{itemize}

    \subsection{Grammars}
    \begin{itemize}
        \item Many tools, e.g.\ NLTK
        \item Ambiguity, several parsing trees
    \end{itemize}

    \subsection{Interpretation}
    \begin{itemize}
        \item \textbf{The 2.\ phase of text understanding}
        \item Knowledge of word meaning and their language use
        \item Result: conceptual graphs, frames, logical program
        \item Check semantics
    \end{itemize}

    \subsection{Use of world knowledge}
    \begin{itemize}
        \item \textbf{The 3.\ phase of text understanding}
    \end{itemize}

    \subsection{Document retrieval}
    \begin{itemize}
        \item Historical: keywords
        \item Now: whole text search
        \item Organize a database, indexing, search algorithms
        \item Input: a query
    \end{itemize}

    \subsection{Document indexing}
    \begin{itemize}
        \item Collect all words from all documents \textbf{(use lemmatization)}
        \item Inverted file
        \item For each word keep:
        \begin{itemize}
            \item The number of appearing documents
            \item Overall number of appearances
            \item For each document:
            \begin{itemize}
                \item Number of appearances
                \item Location
            \end{itemize}
        \end{itemize}
    \end{itemize}

    \subsection{Ranking-based search}
    \begin{itemize}
        \item Web search
        \item Less frequent terms are more informative
        \item NL input --- stop words, lemmatization
        \item Vector-based representation of documents and queries (bag-of-words and dense embeddings)
    \end{itemize}

    \subsection{Vectors and documents}
    \begin{itemize}
        \item A word occurs in several documents
        \item Both words and documents are vectors
        \item A term-document matrix of dimensions $|V| \times |D|$
        \item A sparse matrix
        \item Word embedding
    \end{itemize}

    \subsection{Document similarity}
    \begin{itemize}
        \item Assumes orthogonal dimensions
        \item Dot product of vectors
        \[ \cos(\Theta) = \frac{A \dot B}{|A||B|} \]
    \end{itemize}

    \subsection{Importance of words}
    \begin{itemize}
        \item Frequencies of words in a particular document (and overall)
        \item Inverse document frequency (IDF):
        \[ \text{idf}_b = \log\left(\frac{N}{n_b}\right) \]
        \begin{itemize}
            \item $N$ --- number of documents in collection
            \item $n_b$ --- number of documents with word $b$
        \end{itemize}
    \end{itemize}

    \subsection{Weighting dimensions}
    \begin{itemize}
        \item The weight of word $b$ in document $d$
        \[ w_{b, d} = \text{tf}_{b, d} \times \text{idf}_{b, d} \]
        \begin{itemize}
            \item $\text{tf}_{b, d}$ --- frequency of term $b$ in document $d$
        \end{itemize}
        \item Called \textbf{TF{\_}IDF weighting}
    \end{itemize}

    \subsection{Performance measures for search}
    \begin{itemize}
        \item Statistical measures
        \item Subjective measures
        \item Precision, recall
        \item A contingency table analysis of precision and recall
    \end{itemize}

        \subsubsection{Precision and recall}
        \begin{itemize}
            \item $N$ --- number of documents in the collection
            \item $n$ --- number of inpportant documents for a given query $q$
            \item Search returns $m$ documents, including $a$ relevant ones
            \item \textbf{Precision:} $P = \frac{a}{m}$ --- the proportion of relevant documents in the obtained ones
            \item \textbf{Recall:} $R = \frac{a}{n}$ --- the proportion of obtained relevant documents
        \end{itemize}

        \subsubsection{Other ranking measures}
        \begin{itemize}
            \item \textbf{Precision@k} --- the proportion of relevant documents in the first $k$ obtained ones
            \item \textbf{Recall@k} --- the proportion of relevant documents in the $k$ obtained among all relevant documents
            \item $F_{1}$@k
            \item Mean reciprocal rank:
            \[ MRR = \frac{1}{Q} \sum_{i=1}^{Q} \frac{1}{rank_i} \]
            \begin{itemize}
                \item Over $Q$ queries
                \item Considers only the rank of the best answer
            \end{itemize}
        \end{itemize}

    \subsection{PageRank}
    \begin{itemize}
        \item $p$ --- a web page
        \item $O(p)$ --- pages pointed to by $p$
        \item $I(p) = \{i_1, i_2, \ldots, i_n\}$ --- pages pointing to $p$
        \item $d$ = damping factor between 0 and 1 (default 0.85 or 0.9)
        \item $\pi(p)$ --- page quality, depends on the quality of pages pointing to it
        \[ \pi(p) = (1 - d) + d \frac{\pi(i_1)}{|O(i_1)|} + \cdots + d \frac{\pi(i_n)}{|O(i_n)|} \]
    \end{itemize}

    \subsection{Dense vector embeddings}
    \begin{itemize}
        \item Advantages compared to sparse embeddings:
        \begin{itemize}
            \item Less dimensions, less space
            \item Easier input for ML methods
            \item Potential generalization and noise reduction
            \item Potentially captures synonims
        \end{itemize}
        \item The most popular approaches:
        \begin{itemize}
            \item Matrix-based transformations to reduce dimensionality (SVD and LSA)
            \item Neural embeddings (word2vec, Glove)
            \item Contextual neural embeddings
        \end{itemize}
    \end{itemize}

    \subsection{The word2vec method}
    \begin{itemize}
        \item Instead of \textbf{counting} how often each word $w$ occurs near ``apricot'', train a classifier on a binary \textbf{prediction} task (is $w$ likely to show up near ``apricot''?)
        \item Take the obtained (learned) classifier weights as the \textbf{word embeddings} (groups of similar words)
    \end{itemize}

    \subsection{Contextual embeddings}
    \begin{itemize}
        \item Also take context into account 
        \item ELMo, BERT
    \end{itemize}

        \subsubsection{ELMo}
        \begin{itemize}
            \item Looks at the entire sentence before assigning each word in it an embedding
            \item Predicts \textbf{the next word} in a sequence
            \item Includes subword units
        \end{itemize}

        \subsubsection{BERT}
        \begin{itemize}
            \item Predicts \textbf{masked words} in a sentence
            \item Also predicts the order of sentences
        \end{itemize}

    \subsection{Text summarization}
    \begin{itemize}
        \item Evaluation:
        \begin{itemize}
            \item ROUGE scores
            \item BERTScore
            \item with QA:\ question generation and searching for answers in the summary
        \end{itemize}
        \item Deep neural networks
        \item Short and long texts
    \end{itemize}

    \subsection{Sentiment analysis (SA)}
    \begin{itemize}
        \item A computational study of opinions, sentiments, emotions and attitude expressed in texts towards an entity
    \end{itemize}

    \newpage

\section{Reinforcement learning}
    \newpage

\end{document}
